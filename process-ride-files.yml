# 
# Workflow for reading .fit files in parallel
#

jobs:
# Produces a list of ride files present in the blob container
# within Azure, using Python.
- job: generate_list_of_files
  steps:
  - task: PythonScript@0
    inputs:
      scriptSource: 'inline'
      script: |
        import os, json
        from azure.storage.blob import BlobServiceClient

        CONTAINER_NAME = 'tb-ride-files'

        connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
        service_client = BlobServiceClient.from_connection_string(connect_str)
        container_client = service_client.get_container_client(container=CONTAINER_NAME)

        filenames = {}

        for n, blob in enumerate(container_client.list_blobs()):
          filenames[f'file{n}'] = {'filename', blob.name}

        print(f"##vso[task.setVariable variable=files;isOutput=true]{json.dumps(filenames)}")

    name: get_blob_files
# The previous job generates filenames of the files present
# in the blob container. Now process those files in parallel.
- job: process_ride_file
  dependsOn: generate_list_of_files
  strategy:
    maxParallel: 10
    matrix: $[ dependencies.generator.outputs['get_blob_files.files'] ]
  steps:
  - script: echo $(filename) # echos A or B depending on which leg is running
